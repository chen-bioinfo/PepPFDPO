{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe54389",
   "metadata": {},
   "source": [
    "# 使用训练的模型生成序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5262b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tokenizers import Tokenizer\n",
    "import gc\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(\"your path/tokenizer.json\")\n",
    "BOS_TOKEN_ID = tokenizer.token_to_id(\"<|bos|>\") \n",
    "EOS_TOKEN_ID = tokenizer.token_to_id(\"<|eos|>\")\n",
    "PAD_TOKEN_ID = tokenizer.token_to_id(\"<|pad|>\") \n",
    "\n",
    "CONTEXT = \"<|bos|>\"  # 使用抗菌肽控制符\n",
    "\n",
    "GENERATION_PARAMS = {\n",
    "    \"max_length\": 50,  # 最长生成长度\n",
    "    \"do_sample\": True,  # 是否使用采样生成\n",
    "    \"top_p\": 0.9,       # nucleus sampling 参数\n",
    "    \"temperature\": 1.2,   # 温度参数\n",
    "    \"pad_token_id\": PAD_TOKEN_ID,  # 填充 token ID\n",
    "    \"eos_token_id\": EOS_TOKEN_ID   # 结束 token ID\n",
    "}\n",
    "def cleaned_sequence(seqs):\n",
    "    VALID_AMINO_ACIDS = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    s = []\n",
    "    for seq in seqs:\n",
    "        cleaned = ''.join(char for char in seq.upper() if char in VALID_AMINO_ACIDS)\n",
    "        s.append(cleaned)\n",
    "    return s\n",
    "def main():\n",
    "    # 加载基础模型\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"hugohrban/progen2-large\", trust_remote_code=True).to(DEVICE)\n",
    "        \n",
    "    # 加载 LoRA 适配器\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        \"your path/best_model\"\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # 编码输入\n",
    "    input_ids = torch.tensor(\n",
    "        tokenizer.encode(CONTEXT).ids,\n",
    "        device=DEVICE\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    clean_sequences = []\n",
    "    batch_size = 100  # 每次生成的序列数量\n",
    "    total_sequences = 5000  # 总共生成的序列数量\n",
    "    num_batches = total_sequences // batch_size  # 计算批次\n",
    "\n",
    "    for batch_index in range(num_batches):\n",
    "        with torch.no_grad():  # 禁用梯度计算\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                num_return_sequences=batch_size,  # 当前批次生成数量\n",
    "                repetition_penalty=1.2,  # 惩罚重复token\n",
    "                no_repeat_ngram_size=2,  # 避免重复的n-gram\n",
    "                **GENERATION_PARAMS\n",
    "            )\n",
    "        # print(outputs)\n",
    "        \n",
    "        for output in outputs:\n",
    "            tokens = output.cpu().numpy().tolist()\n",
    "            # 查找 EOS 的位置，若不存在则取全部\n",
    "            eos_pos = tokens.index(EOS_TOKEN_ID) if EOS_TOKEN_ID in tokens else len(tokens)\n",
    "            # 移除 BOS 和 EOS，保留中间 token\n",
    "            valid_tokens = tokens[1: eos_pos]  # 去除 BOS\n",
    "            if not valid_tokens:  # 跳过空序列\n",
    "                continue\n",
    "            # 转换为字符串并移除 PAD\n",
    "            seq = tokenizer.decode(valid_tokens).replace(str(PAD_TOKEN_ID), \"\")\n",
    "            clean_sequences.append(seq)\n",
    "        \n",
    "        # 清理显存\n",
    "        del outputs\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Batch {batch_index + 1}/{num_batches} completed.\")\n",
    "    rs = cleaned_sequence(clean_sequences)\n",
    "    # print(rs)\n",
    "    # 保存生成的序列到 CSV 文件\n",
    "    df = pd.DataFrame({'Sequence': rs})\n",
    "    df.to_csv('your path/sequence.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ae147",
   "metadata": {},
   "source": [
    "# 评估生成的序列的AMP、MIC、Toxic、Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c176a680",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from amp.utils import basic_model_serializer\n",
    "import amp.data_utils.sequence as du_sequence\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..')))\n",
    "from toxinpred3.toxic import ToxinPred3\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# 加载基础模型\n",
    "#加载评估perplexity模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\"hugohrban/progen2-large\", trust_remote_code=True).to(device)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hugohrban/progen2-large\", trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 初始化预测器 加载打分模型\n",
    "# 加载毒性模型\n",
    "toxic_predictor = ToxinPred3(threshold=0.5, model=1)\n",
    "\n",
    "# 加载AMP、MIC评估模型\n",
    "bms = basic_model_serializer.BasicModelSerializer()\n",
    "amp_classifier = bms.load_model('/geniusland/home/wanglijuan/sci_proj/models/amp_classifier')\n",
    "amp_classifier_model = amp_classifier()\n",
    "mic_classifier = bms.load_model('/geniusland/home/wanglijuan/sci_proj/models/mic_classifier/')\n",
    "mic_classifier_model = mic_classifier()\n",
    "\n",
    "\n",
    "df = pd.read_csv('your path/sequence.csv')\n",
    "seqs = df['Sequence'].tolist()\n",
    "pad_seq = du_sequence.pad(du_sequence.to_one_hot(seqs))\n",
    "pred_amp = amp_classifier_model.predict(pad_seq)\n",
    "pred_mic = mic_classifier_model.predict(pad_seq)\n",
    "amp_list = []\n",
    "mic_list = []\n",
    "toxic_list = []\n",
    "perplexity_list = []\n",
    "for sequence in tqdm(seqs):\n",
    "        input_ids = tokenizer.encode(sequence, return_tensors=\"pt\", padding=True, truncation=True, max_length=64).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "            \n",
    "            perplexity_list.append(perplexity.item()/100)\n",
    "for s in tqdm(seqs):\n",
    "    r_tox = toxic_predictor.predict_sequence(s)\n",
    "    toxic_list.append(r_tox['ML Score'])\n",
    "for i in range(len(seqs)):\n",
    "    amp_list.append(pred_amp[i][0])\n",
    "    mic_list.append(pred_mic[i][0])\n",
    "\n",
    "data = {\n",
    "    'Sequence': seqs,\n",
    "    'AMP': amp_list,\n",
    "    'MIC': mic_list,\n",
    "    'Toxic': toxic_list,\n",
    "    'Perplexity': perplexity_list\n",
    "}\n",
    "# print(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('your path/sequence.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c331da",
   "metadata": {},
   "source": [
    "# 成功率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e6442",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义文件夹路径和输出文件路径\n",
    "folder_path = \"your path\"  # 替换为你的文件夹路径\n",
    "output_file = \"your path/success_rate_results.csv\"  # 输出文件名\n",
    "\n",
    "# 获取文件夹中所有 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 定义一个函数来计算成功率\n",
    "def calculate_success_rate(data):\n",
    "    # 筛选满足条件的序列\n",
    "    successful_sequences = data[\n",
    "        (data['AMP'] > 0.8) & \n",
    "        (data['MIC'] > 0.8) & \n",
    "        (data['Toxic'] < 0.38)\n",
    "    ]\n",
    "    # 计算成功率\n",
    "    success_rate = len(successful_sequences) / len(data) if len(data) > 0 else 0\n",
    "    return success_rate\n",
    "\n",
    "# 初始化结果字典\n",
    "results = {}\n",
    "\n",
    "# 遍历每个 CSV 文件并计算成功率\n",
    "for csv_file in csv_files:\n",
    "    # 读取文件\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # 确保包含 AMP, MIC, Toxic 列\n",
    "    if all(col in data.columns for col in ['AMP', 'MIC', 'Toxic']):\n",
    "        # 计算成功率\n",
    "        success_rate = calculate_success_rate(data)\n",
    "    else:\n",
    "        print(f\"文件 {csv_file} 缺少 AMP, MIC 或 Toxic 列，跳过。\")\n",
    "        success_rate = None  # 如果缺少列，则标记为 None\n",
    "    \n",
    "    # 保存结果（列名为文件名去掉路径和后缀）\n",
    "    column_name = os.path.splitext(csv_file)[0]\n",
    "    results[column_name] = [success_rate]\n",
    "\n",
    "# 将结果保存到 CSV 文件\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(folder_path, output_file), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207368b",
   "metadata": {},
   "source": [
    "# Diversity 计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5496b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "# 定义文件夹路径\n",
    "folder_path = \"your path\"  # 替换为你的文件夹路径\n",
    "output_file = \"your path/edit_distance.csv\"  # 输出文件名\n",
    "\n",
    "# 获取文件夹中所有 CSV 文件\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "# 找到原始数据集文件（假设命名为 \"original.csv\"）\n",
    "original_file = [f for f in csv_files if 'src' in f.lower()]\n",
    "if not original_file:\n",
    "    raise FileNotFoundError(\"未找到原始数据集文件（文件名中包含 'src' 的文件）\")\n",
    "original_file = original_file[0]\n",
    "\n",
    "# 读取原始数据集\n",
    "original_data = pd.read_csv(os.path.join(folder_path, original_file))\n",
    "original_sequences = original_data.iloc[:, 0].astype(str).tolist()  # 假设原始数据集的序列在第一列\n",
    "\n",
    "# 初始化结果字典\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# 遍历其他 CSV 文件\n",
    "for csv_file in csv_files:\n",
    "    if csv_file == original_file:\n",
    "        continue  # 跳过原始数据集文件\n",
    "\n",
    "    # 读取当前文件\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    current_data = pd.read_csv(file_path)\n",
    "    current_sequences = current_data.iloc[:, 0].astype(str).tolist()  # 假设序列在第一列\n",
    "\n",
    "    # 计算每条序列的编辑距离（与原始数据集中最小的编辑距离）\n",
    "    min_distances = []\n",
    "    for seq in current_sequences:\n",
    "        distances = [levenshtein_distance(seq, orig_seq) for orig_seq in original_sequences]\n",
    "        min_distances.append(min(distances))\n",
    "\n",
    "    # 保存结果（列名为文件名去掉路径和后缀）\n",
    "    column_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    results_df[column_name] = pd.Series(min_distances)\n",
    "\n",
    "# 将结果保存到 CSV 文件\n",
    "# results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(folder_path, output_file), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
